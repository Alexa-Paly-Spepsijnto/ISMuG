# ISMuG

## Overview & Background
Our interactive smart music generator employs novel AI music generation and interactive feedback systems based on a user’s human state, which is detected via computer vision and physiological sensors like a heart rate monitor or galvanic skin response sensor. One of the main reasons we came up with this idea was because of our shared interests in both machine learning and its applications and music in general. We also thought this project would serve as a great proof of concept for companies like startups for autonomous cars, which are currently working on interactive music recommendation systems based on the passengers’ emotional states as monitored through computer vision. Our project can additionally be used as an open-ended tool for other research pertaining to the psychology of music and its effect on people’s emotions, which is an area that’s widely researched and of a lot of interest in the research world.

Both before and during our project work, we read a lot of the literature pertaining to the intersection of music or art and AI. Emotions are often vague, complex, and ambiguous so it’s hard to adjust generated art and music to match a subjective taste and incorporate preference learning with that. Since our project revolves around identifying features in the music that we can tweak to ensure it reflects the user’s mood, we read about features like dynamics, pitch, rhythm, and things like that to see how that influences people’s emotions. In a lot of these prior works, the ground-truth mood of a piece of music was typically extracted via low-level audio features like the ones mentioned, along with metadata about the piece. 

## Valence-Arousal Model
Our valence-arousal model for modeling human state contains four quadrants: anxiety, exuberance, depression, and contentment. Valence corresponds to the pleasure or displeasure tied to a particular emotion, while arousal corresponds to the level of positive or negative feeling associated with those emotions. High arousal and high valence indicates a very pleased and positive emotion, thus resulting in the exuberance quadrant. Similarly, low arousal and low valence indicates a negative and displeased state of depression. 

## Human State Detection
Computer Vision
The main component of human state detection in our project is facial expression recognition using Computer Vision. There is a dataset called FER+ with labeled faces and facial expressions. We found a few pre-trained models trained on this dataset and tested them by using it on ourselves. We found the best model as part of the FER Python library, an example of which is shown. The model detects neutral, happy, sad, angry, etc, but the reality was that happy and neutral were the only “naturally” occurring expressions, whereas we have to intentionally try to trigger the other labels. This is a shortcoming we are aware of.

## Physiological Sensors
The other component we tried for human state detection is collecting heart rate using a pulse oximeter. We used an arduino with a pulse oximeter. A sample reading is shown – it’s very noisy, so we used a Kalman filter and did peak detection to get the heart rate. While we were able to discern the heart rate from the noisy data, the sensor was very sensitive to movement of the finger, so it had to be placed exactly right and pressed down and held still. It was too unreliable to continue using.

## Novel Music Generation

### TensorFlow Magenta
The second component of our project is novel music generation. We began by looking into Tensorflow’s existing music generation models, which are part of Magenta. We found three models that were of interest to us: ImprovRNN, MelodyRNN, and PerformanceRNN. Each of these models typically takes in some sort of primer note or melody or MIDI file and does various types of music generation, whether that’s a melody line or chords or just enhancing and making a prior melody more complex. The models are all pre-trained on a large dataset of MIDI files, and they include configurable parameters like the number of outputs per input, a primer melody, backing chords, and a couple other configurations. We also made an attempt at training our own models instead of using Magenta models. We utilized a dataset called EMOPIA, which is a multi-modal pop piano dataset that contains over 1000 clips annotated with 4 different mood categories. We made use of Magenta’s tutorials on training a custom model and tried to work with adjusting the batch sizes, layer sizes, and eval ratios. This custom model training unfortunately didn’t work as expected due to some issues with the bundle files for our model, so we gave a more detailed look into using the pre-trained ImprovRNN and PerformanceRNN Magenta models. 

### Existing Pipeline
First, we manually created some chord progressions that we self-labeled with one of the four categories as described in the valence-arousal chart. After that, we used Markov chains to perform data augmentation and increase the size of our chord dataset. We had all these chords and their respective labels stored, so once we knew what state a user was in, we randomly chose one of the corresponding chords with that state label to use as an input to Magenta’s ImprovRNN model. ImprovRNN generates a melody line for a provided set of backing chords, so after running ImprovRNN, we had a MIDI file containing the chords we passed as input and a generated melody line. This melody line was usually too simple and fairly boring, so we decided to utilize another model called PerformanceRNN to enhance the melody. We split the output of ImprovRNN into 2 separate MIDI files – one for the chords and one for the melody – and passed the melody MIDI track to PerformanceRNN for it to enhance it. Once that was done, we took the output of PerformanceRNN, the enhanced melody line, and recombined it with the original chords we had provided as input to ImprovRNN. When we listened to sample outputs, the melody line didn’t really seem to relate to the chords at all and it was difficult to even tell what mood the music was supposed to reflect, so we weren’t really happy with this version of our pipeline. 

### Replicate
While working on those models, we also came across a library called Replicate, which is a music classifier based on an open-source music and audio analysis library called Essentia. There are two main things we thought would be useful in Replicate. First, they provide an API that allows you to see quantified measurements about things like the sadness, happiness, danceability, genre, and things like that about a particular piece of input. Second, they provide another API that plots the valence-arousal level of the input on a chart much like the one we use. We decided that this second API of a valence-arousal graph would be useful in checking whether or not our generated output matches the mood we expected to generate. 

### Custom Model Training
We decided to try creating a primer melody generator using preference learning to adjust our inputs that could be added on to our existing system. As shown in the diagram, we also interface with Replicate to confirm whether our output matches the detected mood of the user, and we implement a user feedback system upon audio playback to determine whether our generated music matches the users’ preferences. 

## EMOPIA Dataset
We use the EMOPIA dataset to create our own ML model to generate primer melodies. EMOPIA data is already categorized into the four valence-arousal quadrants we wanted to work with. All music is located on a single track (i.e. the melody and chords are not separated) – this, however, leads to complications in developing a custom RNN model. MIDI also provides a lot of information and features for us to use. We selected pitch, velocity, and note duration since these are strongly correlated to arousal and valence values. We used Magenta’s NoteSequence Protobuf data format to manipulate MIDI files such that we can see information about a note’s pitch, start/end times, and velocity. 

### Markov Decision Process (MDP) 
Our first attempt to generate a primer melody was through the Markov Decision Process (MDP). Markov Chains are used to predict future states based on current state; in this case, we can predict the next note based on our current note. We parsed the MIDI files in EMOPIA and extracted pitch, velocity, and note duration. Then, we generated tokens for MDP. Specifically, we used bigrams, which are a pair of starting and next state. More complex use cases can utilize n-grams, with n states rather than just a pair. Additional tuning included limiting the pitch range of a note to a single octave and fixing the note duration to quarter notes for the sake of simplicity. For further work, we would use MDP to predict velocity and note duration as well. However, Markov Chains have no memory and thus the notes can sound disjoint without an ability to look into the past to influence our future. As such, MDP cannot effectively create believable patterns for music. 

### RNN & LSTM
Recurrent Neural Networks (RNNs) are currently the most prominent state-of-the-art model being used to generate music; in fact, Magenta uses RNNs for most, if not all, of their models. Our current RNN model training involves treating notes like a text corpus, where each note can lead to several other notes. However, unlike text, time is involved; notes can happen at the same time, in the middle of another note, or while several notes are also being continuously played at the same time. This introduces another layer of complexity of processing the MIDI in such a way that we only extract the melody from our data. Unfortunately, as previously noted, EMOPIA contains everything on a single track. Thus, we need another method of parsing these MIDI tracks. 

### User Feedback
A main goal of our project is to follow the user’s state and try to match it at all times. This challenge also involves preference learning because, for any given state, different users might prefer different kinds of music. For each song, we want to prompt the user to give feedback. One way we could do that is the absolute way, which is to ask how satisfied the user is with the current song. Another way is a relative way where we ask how the user prefers the current song to the previous one. The latter method is more useful for our application because it lets us rank the songs played so far more easily. We want the user feedback to affect how the inputs for our melody generation are chosen. These would be pitch, velocity, and duration. The rankings would keep track of what input combination that the user preferred for each state. This information is passed back to music generation to either use the preferred or exploit the information and try new combinations to gather new rankings.

## Remaining Work
We have some refinement to do in our preference learning implementation. Specifically, we would like to include a ranking system such that the user can rank each generated output relative to ones previously played. Additionally, we would like to complete our implementation of a custom RNN for better music generation. Since MDP doesn’t have memory, RNN/LSTMs are better at determining and creating musical patterns based on past events. Lastly, we would like to collect survey data using our finalized system, so as to evaluate its performance and make any further improvements and adjustments. 
